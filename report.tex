\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[a4paper, portrait, margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{tcolorbox} % for creating colored boxes

\newcommand{\K}{\mathbb{K}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\H}{\mathbb{H}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\ii}{\mathrm{i}}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\weakcv}{\overset{D}{\rightarrow}}
\newcommand{\probcv}{\overset{\mathbb{P}}{\rightarrow}}
\newcommand{\ascv}{\overset{a.s.}{\rightarrow}}
\newcommand{\diff}{\ensuremath{\operatorname{d}\!}}
\newcommand{\dv}[1]{\frac{\diff}{\diff #1}}

\DeclareMathOperator*{\argmax}{\arg,max}
\DeclareMathOperator*{\argmin}{\arg,min}

\newenvironment{problem}[2][Problem]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{solution}[1][\proofname]{%
  \proof[Solution]%
}{\endproof}

\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newtcolorbox[auto counter,number within=section]{notebox}[1][]{
  colback=purple!5!white,
  colframe=purple!55!black,
  fonttitle=\bfseries,
  title=Note~\thetcbcounter: #1,
}

\newtcolorbox[auto counter,number within=section]{summarybox}[1][]{
  colback=green!5!white,
  colframe=green!75!black,
  fonttitle=\bfseries,
  title=Summary~\thetcbcounter: #1,
}

\newenvironment{note}[1]
{\begin{notebox}[#1]\begin{quote}}
  {\end{quote}\end{notebox}}

  \newenvironment{summary}[1]
  {\begin{summarybox}[#1]\begin{quote}}
  {\end{quote}\end{summarybox}}


\title{C8.2 Stochastic Analysis and PDEs project \\ Branching processes and semilinear parabolic PDEs}
\author{Matthieu Meunier}
\date{\today}

\begin{document}

\maketitle

Representing solutions of PDEs with probabilistic objects, such as diffusion processes, presents many advantages both from the theoretical and practical point of view. Among others, such a probabilistic representation allows to use stochastic algorithms (e.g. Monte Carlo methods) to numerically approximate solutions of high-dimensional PDEs without necessarily incurring an exponential cost in the dimension of the problem (the so-called curse of dimensionality).

This report proposes to take a deeper dive into the relationship between semilinear parabolic PDEs and branching processes, thus extending section 7.4 of the lecture notes. Our study will mostly rely on work by Labordere et al. \cite{labordere2019branching}. In a nutshell, they are able to show an extended version of the classical representation for KPP equations (e.g. Theorem 7.32 of the lecture notes) to the case of gradient non-linearities, i.e. equations of the form 
 \[
 - \partial_t u - \mathcal L u = f(u, \nabla_x u), \quad u(T,x) = g(x), \quad x \in \R ^{d}, t \in [0,T]
 \]

 \section{An introductory example}
 \label{section:intro}
We present the main idea of the article in the context of a simple 1-dimensional equation 
 \begin{equation}
	 \label{eq:pde-example}
	 - \partial_t u - \frac{1}{2}\partial_{xx} u = \frac{1}{2}(u ^{2} + u \partial_x u), \quad u(T,x) = g(x), \quad x \in \R, t \in [0,T]
 \end{equation}

The Feynman-Kac formula allows to write the solution at initial time as follows
 \[
 u(0,x) = \mathbb E \left[ g(W_T) + \int_{0}^{T} f(u, \partial_x u)(t,W_t) \diff t \mid W_0 = x \right]
 \]
 where $(W_t)$ is a Brownian motion (not necessarily started at $0$). Now, consider a non-negative random variable $\tau ^{1}$ with density $\rho > 0$ on $\R_+$, as well as a Brownian motion $W ^{1}$ independent of $\tau ^{1}$. Let $\overline{F}(t) = \int_{t}^{\infty} \rho(s) \diff s$. Then one can rewrite the Feynman-Kac formula in terms of the distribution of $\tau ^{1}$
 \[
	 u(0,x) = \mathbb E _{0,x} \left[ \overline{F}(T) \frac{g(W_T ^{1})}{\overline{F}(T)} + \int_{0}^{T} \frac{f(u,\partial_x u)(t,W ^{1}_t)}{\rho(t)}\rho(t) \diff t \right] = \mathbb E _{0,x} \left[ \phi(0,\tau ^{1} \wedge T, W ^{1}_{\tau ^{1} \wedge T}) \right]
 \]
 where $\phi(s,t,y) := \frac{\mathbf{1} _{t \geq T}}{F(t - s)}g(y) + \frac{\mathbf{1} _{t < T}}{\rho(t - s)}(u \partial_x ^{I_1} u)(t,y)$ and $I_1$ is a Bernoulli random variable independent of $\tau ^1$ and $W ^{1}$. Here $\partial_x ^{1} u  = \partial_x u$ and $\partial_x ^{0}u = u$.

By the Markov property, when $I_1 = 0$, we get
 \begin{align*}
	 u \partial_x ^{I_1} u (t,y) = u(t,y) ^{2} &= \mathbb E _{t,y} \left[ \phi(t, (t + \tau ^{1}) \wedge T, W ^{1}_{(t + \tau ^{1}) \wedge T}) \right]^{2} \\
						   & = \mathbb E _{t,y} \left[ \phi(t, \tau ^{1,1}_{t}, W ^{1,1}_{\tau _{t} ^{1,1}}) \phi(t, \tau ^{1,2}_{t}, W ^{1,2}_{\tau _{t} ^{1,2}}) \right]
 \end{align*}
 where $W ^{1,i}, \tau ^{1,i}_t := (t + \tau ^{1,i}) \wedge T$ are all independent, $\tau ^{1,i}$ are distributed according to $\rho$ and $W ^{1,i}$ are Brownian motions for $i = 1,2$. Here, $\mathbb E _{t,y}$ means expectation conditioned on $W_t ^{1,1} = W_t ^{1,2} = y$.

 When $I_1 = 1$, we get 
 \[
	 u \partial_x ^{I_1} u (t,y) = \mathbb E _{t,y} \left[ \phi(t,\tau ^{1,1}_{t}, W ^{1,1}_{\tau ^{1,1}_{t}}) \right] \partial _{y} \mathbb E _{t,y} \left[\phi(t,\tau ^{1,2}_{t}, W ^{1,2}_{\tau ^{1,2}_{t}}) \right]
 \]

 Expanding the second conditional expectation and differentiating inside the heat kernel gives
 \[
	 \partial_y \mathbb E _{t,y} \left[ \phi(t,\tau ^{1,2}_{t}, W ^{1,2}_{\tau ^{1,2}_{t}}) \right] = \mathbb E _{t,y} \left[\frac{W ^{1,2}_{\tau ^{1,2}_t} - W ^{1,2}_{t}}{\tau ^{1,2}_{t} - t} \phi(t,\tau ^{1,2}_{t}, W ^{1,2}_{\tau ^{1,2}_{t}}) \right]
 \]
 Introducing $\mathcal W ^{1} := \mathbf{1} _{I_1 = 0} + \mathbf{1} _{I_1 = 1} \frac{\Delta W _{T _{1,2}}^{1,2}}{\Delta T _{(1,2)}}$, where $\Delta W ^{1,2}_{T _{(1,2)}}:= W ^{1,2}_{T _{(1,2)}} - W ^{1,2}_{T _{(1)}}$, $\Delta T _{(1,2)} = T _{(1,2)} - T _{(1)}$ and $T _{(1,i)} = (\tau ^{1} + \tau ^{1,i}) \wedge T$, yields the following representation of the solution of \eqref{eq:pde-example}
\begin{equation}
    \label{eq:intro-ex-branching-rep}
    \begin{aligned}
    &u(0,x) = \\ &\mathbb E _{0,x} \left[ \mathbf{1} _{T _{(1)} = T} \frac{g(W_T)}{\overline{F}(T)} + \mathbf{1} _{T _{(1)} < T} \mathcal W ^{1}  \prod _{i = 1}^{2} \left( \mathbf{1} _{T _{(1,i)} = T} \frac{g(W _{T}^{1,i})}{\overline{F}(\Delta T _{(1,i)})} + \mathbf{1} _{T _{(1,i)} < T} \frac{u \partial_x ^{I ^{1,i}} u (T _{(1,i)}, W ^{1,i}_{T _{(1,i)}})}{\rho(\Delta T _{(1,i)})} \right) \right]
\end{aligned}
\end{equation}

One might wonder why this representation is interesting. Essentially, we can repeat the trick of expressing the non-linearity $u \partial_x ^{I} u$ using random times $T _{(1,i,j)} = (\tau ^{1} + \tau ^{1,i} + \tau ^{1,i,j})\wedge T$, and so on. As such, the term carrying the non-linearity becomes negligeable on average, since the events $\{T _{(1,i_2,\cdots,i_k)} < T \}$ become less likely. If $\mathcal W$ can be computed easily and has nice integrability properties, then we can reasonably hope that this representation is helpful.

We will now make this argument more general, considering more general polynomial non-linearities and diffusion processes. The setting of our study is presented in Section \ref{section:pde-polynomial-non-linear}. 
To formalize this splitting trick, we turn to age-dependent marked branching processes in Section \ref{section:branching-processes}, and then state the representation result in Section \ref{section:representation-result}. Some numerical experiments are carried out in Section \ref{section:numerical-experiments}. 

\section{Semilinear parabolic PDEs with polynomial gradient non-linearity}
\label{section:pde-polynomial-non-linear}

Let $d \geq 1$, $\mu: [0,T]\times \R ^{d} \rightarrow \R ^{d}$, $\sigma: [0,T] \times \R ^{d} \rightarrow \R ^{d \times d}$. For $u: [0,T]\times \R ^{d} \rightarrow \R$, we denote by $\nabla u$, $\nabla ^{2} u$ the gradient and Hessian respectively of $u$ with respect to the space variable $x$. Let $m \geq 0$, and consider a subset $L \subset \N ^{m + 1}$ along with a sequence of functions $(c _{l})_{l \in L}$ and $(b_i)_{i \in \{1, \cdots, m\}}$ where $c_l : [0,T]\times \R ^{d} \rightarrow R$ and $b_i :[0,T] \times \R ^{d} \rightarrow \R ^{d}$. For $l \in L$ let $|l| = \sum_{i = 0} ^{m} l_i$. We define the generator function $f: [0,T]\times \R ^{d}\times \R \times \R ^{d}$ by
\begin{equation}
	\label{eq:generator}
    f(t,x,y,z) := \sum _{l \in L} c_l(t,x)y ^{l_0} \prod _{i = 1}^{m} (b_i(t,x) \cdot z)^{l_i}
\end{equation}

and we consider, for such a generator, the following PDE
 \begin{equation}
	 \label{eq:pde}
 \partial_t u + \mu \cdot \nabla u + \frac{1}{2}\mathrm{Trace}(\sigma \sigma ^{T}\nabla ^{2}u) + f(\cdot,u,\nabla u), \quad (t,x) \in [0,T)\times \R ^{d}, \quad u(T,\cdot) = g
 \end{equation}
 for some terminal condition $g : \R ^{d} \rightarrow \R$ assumed to be Lipschitz and bounded.

\section{Age-dependent marked branching processes.}
\label{section:branching-processes}
\subsection{Background and notations}

Consider a fixed finite time horizon $0 < T < \infty$.
We consider a branching time characterized by a splitting time density $\rho$ and a probability mass function $(p_l)_{l \in L}$ where $L \subset \N ^{m + 1}$. For $l \in L$, let $|l| = \sum_i l_i$. $m \in \N_0$ characterizes the different marks assigned to particles. At the splitting time, the particle branches into $|l|$ offsprings with probability $p_l$, among which the first $l_0$ carry the mark $0$, the next $l_1$ carry the mark $1$ and so forth. Then, regardless of its mark, each particle performs the same independent branching process.
Let $(\Omega, \mathcal F, \mathbb P)$ be a proability space carrying a sequence of iid positive random times $(\tau ^{n,q})_{n,q \geq 1}$ of density $\rho: \mathbb R \rightarrow \mathbb R _{+}$ supported on $\R _{+}$, and a sequence of iid $L$-valued random variables $(I ^{n,q})_{n,q \geq 1}$ such that $\mathbb P (I ^{n,q} = l) = p_l $ for all $l \in L$. We assume that the families $(\tau ^{n,q})$ and $(I ^{n,q})$ are independent.

We now construct an age-dependent branching process, where each particle is assigned a unique tuple label $ k \in \bigcup_n \mathbb N ^{n}$. Consider for each $n$ an injection $\pi_n : \N ^{n} \rightarrow \N$.
\begin{itemize}
    \item The first particle is marked by $0$, labelled by $(1)$, and is said to be of generation $1$. Its splitting time is given by $T _{(1)} := \tau ^{1,1} \wedge T$;
    \item Given a particle of generation $n$ labelled by $k = (k_1, k_2, \cdots, k_n) \in \N ^{n}$, and with splitting time $T _{k}$, we let $I_k = I ^{n, \pi_n (k)}$. If $T_k < T$, the particle is split into $|I_k|$ offsprings, and each are assigned a label $(k_1, \cdots, k_n, i)$ for $i = 1, \cdots, |I_k|$, otherwise it simply dies at $T$.
    \item Given $I_k = l = (l_0, \cdots, l_m)$, the first $l_0$ are assigned the mark $0$, the next $l_1$ are assigned the mark $1$, up until the last $l_m$ which are assigned the mark $m$.
    \item For a particle $k = (k_1, \cdots, k_{n+1})$ of generation $(n+1)$, we denote by $k- = (k_1, \cdots, k_n)$ its parent, hence the splitting time of $k$ is given by $T_k = (T _{k-} + \tau ^{n+1, \pi_{n+1}(k)})\wedge T$.
	    \item In partiular, for the initial particle, $k- = \emptyset$ and $T _{k-} = 0$.
\end{itemize}

Finally, we introduce $\theta_k \{0, 1, \cdots, m\}$ to be the mark of $k$ and
 \[
 \mathcal K ^{n}_{t} := \begin{cases}
	 \{ k \text{ of generation }n : \; T_{k-} \leq t < T _{k} \}, & \text{when }t \in [0,T), \\
	 \{ k \text{ of generation } n : \; T_k = T \} & \text{ when } t = T,
 \end{cases}
 \]
 as well as $\overline{\mathcal K}_{t} ^{n} := \bigcup _{s \leq t} \mathcal K ^{n}_{s}$, $\mathcal K _{t} = \bigcup_n \mathcal K ^{n}_{t}$, and $\overline{\mathcal K}_{t} := \bigcup_n \overline{\mathcal K} _{t}^{n}$.
 Clearly, $\mathcal{K}_t$ (resp. $\mathcal{K}_t^n$ ) denotes the set of all living particles (resp. of generation $n$) in the system at time $t$, and $\overline{\mathcal{K}}_t$ (resp. $\overline{\mathcal{K}}_t^n$) denotes the set of all particles (resp. of generation $n$ ) which are alive at time $t$ or have been alive before $t$.

 It can be shown that under the assumption that $\sum_l |l|p_l < \infty$ and that the splitting time distribution does not have any mass at $0$ (which is implied by our definition with a density function), then $\overline{\mathcal K}_t$ is finite almost-surely for all $t$.

 \subsection{The marked branching diffusion}

 Consider a family of $d$-dimensional Brownian motions $(W ^{n,q})$ which are independent of $(\tau ^{n,q},I ^{n,q})$. Define $W _{t}^{(1)} = \Delta W _{t}^{(1)} := W ^{1,1}_{t}$ for $t \in [0,T ^{(1)}]$, and for $k = (k_1, \cdots, k_n) \in \overline{K}_{T} \backslash \{(1)\}$, define
 \[
	 W _{t}^{k} := W ^{k-}_{T _{k-}} + \Delta W ^{k}_{t - T _{k-}}, \quad \text{where } \Delta W ^{k}_{t - T _{k -}}:= W ^{n, \pi_n(k)}_{t - T _{k-}}, \; \text{ for all $t \in [T _{k-}, T_k]$}
 \]

 For each $k \in \overline{\mathcal K}_T$ we define an associated diffusion process
 \begin{equation}
	 \label{eq:sde-diffusions}
 X_t^k=X_{T_{k-}}^{k-}+\int_{T_{k-}}^t \mu(s, X_s^k) d s+\int_{T_{k-}}^t \sigma(s, X_s^k) d W_s^k, \quad t \in\left[T_{k-}, T_k\right], \mathbb{P} \text {-a.s. }
 \end{equation}
where the initial condition for $(1)$ is $X ^{(1)}_0 = x_0 \in \R ^{d}$.
Finally, we introduce the $\sigma$-fields $\mathcal F _{0} := \sigma \{ \tau ^{n,q}, I ^{n,q} : \; n,q \geq 1 \}$ and $\mathcal F _{n} := \sigma \{ W ^{i,q},\tau ^{i,q}, I ^{i,q} : \; q \geq 1, i \leq n \}$ for all $n \geq 1$.

 \section{The representation result}
 \label{section:representation-result}

We work under the following assumptions:
\begin{itemize}
	\item $(p_l)$ satisfies $p_l > 0$ for all $l \in L$ and $\sum_l |l|p_l < \infty$. $\rho$ is continuous and strictly positive on $[0,T]$, and such that $\overline{F}(T) = \int_{T}^{\infty} \rho(t) \diff t > 0$. 
	\item The coefficients of the diffusion $\mu,\sigma$ are bounded continuous and Lipschitz in space.
	\item $c_l,b_i$ are bounded continuous for all $l\in L, i\in \{0,\cdots, m\}$. In particular, $f$ defined by \eqref{eq:generator} is continuous.
	\item There is a measurable functional $\overline{\mathcal W}(t,s,x,(W_r - W_t) _{r \in [t,s]})$ such that $(t,x) \mapsto \overline{\mathcal W}(t,s,x,(W_r - W_t) _{r \in [t,s]})$ is continuous, and for any $s \in [t,T]$ and bounded measurable function $\phi : \mathbb R ^{d} \rightarrow \R$, one has
		\begin{equation}
			\label{eq:malliavin-assumption}
			\partial_x \mathbb E \left[ \phi( \overline{X} ^{t,x}_{s}) \right] = \mathbb E \left[ \phi(\overline{X} ^{t,x}_{s}) \overline{\mathcal W}(t,s,x,(W_r - W_t) _{r \in [t,s]})\right]
		\end{equation}
\end{itemize}

The functional $\overline{W}$ is referred to as a Malliavin automatic differentiation function, and is obtained in practice through Malliavin calculus techniques. Note that in the case of $\mu,\sigma$ being constants, we can use 
 \[
 \overline{\mathcal{W}}\left(t, s, x,\left(W_r-W_t\right)_{r \in[t, s]}\right):=\left(\sigma_0^{\top}\right)^{-1} \frac{W_s-W_t}{s-t} .
 \]

 By analogy with the example of Section \ref{section:intro}, we introduce
 \[
	 \mathcal W _{k} := \mathbf{1} _{\theta_k = 0} + \mathbf{1} _{\theta_k \neq 0} b _{\theta_k}(T _{k-},X ^{k}_{T _{k-}}) \cdot \overline{\mathcal W}(T _{k -}, T _{k}, X ^{k}_{T _{k-}}, \Delta W ^{k}).
 \]
 for all particles $k \in \overline{\mathcal K}_T$. For $u \in C ^{1,2}([0,T]\times \R ^{d})$ we consider the following $n$-th generation decomposition
 \begin{equation}
	 \label{eq:nth-decomposition}
\begin{aligned}
	\psi_n := & \left[ \prod _{k \in \cup_{j = 1}^{n} \mathcal K _{T}^{j} } \frac{g(X ^{k}_{T}) - g(X ^{k}_{T _{k-}})\mathbf{1} _{\theta_k \neq 0}}{\overline{F}(\Delta T_k)}\mathcal W _{k} \right]  \left[ \prod _{k \in \cup _{j = 1}^{n}(\overline{\mathcal K}^{j}_{T} \backslash \mathcal K ^{j}_{T})} \frac{c _{I_k}(T _{k}, X ^{k}_{T _{k}})}{p _{I_k}} \frac{\mathcal W _{k}}{\rho(\Delta T _{k})} \right] \\
		  & \times \left[ \prod _{k \in \overline{\mathcal K}^{n+1}_{T}} \left(\mathbf{1} _{\theta _{k} = 0}u + \sum_{i = 1}^{m}\mathbf{1} _{\theta _{k} = i}b_i \cdot \nabla u\right)(T _{k-}, X ^{k}_{T _{k-}}) \right],
\end{aligned}
 \end{equation}
 and the limit 
 \begin{equation}
	 \label{eq:representation}
 \psi:=\left[\prod_{k \in \mathcal{K}_T} \frac{g(X_T^k)-g(X_{T_{k-}}^k) \mathbf{1}_{\theta_k \neq 0}}{\bar{F}(\Delta T_k)} \mathcal{W}_k\right]\left[\prod_{k \in \overline{\mathcal{K}}_T \backslash \mathcal{K}_T} \frac{c_{I_k}(T_k, X_{T_k}^k)}{p_{I_k}} \frac{\mathcal{W}_k}{\rho(\Delta T_k)}\right]
 \end{equation}
 We similarly define $\psi ^{t,x}_{n},\psi ^{t,x}$ when the branching diffusion is started at $X ^{(1)}_t = x$ for $(t,x) \in [0,T]\times \R ^{d}$.
 We now state a version of the representation result (Proposition 3.4 in \cite{labordere2019branching}).
 \begin{theorem}
	 Suppose the above assumptions hold true, and that the PDE \eqref{eq:pde} has a classical solution $u \in C ^{1,2}$ with $\mathbb E \left[ \int_{t}^{T} |f(s,\overline{X}^{t,x}_{s}, u(s,\overline{X}^{t,x}_{s}), \nabla u (s, \overline{X}^{t,x}_{s}) )| \diff s \right] < \infty$ for some initial condition $(t,x) \in [0,T]\times \R ^{d}$. Assume further that the sequence $(\psi ^{t,x}_{n})_n$ is uniformly integrable. Then $\psi ^{t,x}$ is integrable and $u(t,x) = \mathbb E \left[ \psi ^{t,x} \right]$.
 \end{theorem}

The proof is given by an adaptation of the reasoning carried out in Section \ref{section:intro}.
 A more explicit version is given by Theorem 3.5 in \cite{labordere2019branching}, where the conditions for the validity of the representation are more transparent, but it does not provide more insights.

 \section{Numerical experiments}
 \label{section:numerical-experiments}

We are now interested in applying this result to a concrete example. 
We will compare the choice of the authors for the splitting distribution against other ones. The authors propose to choose a gamma distribution $\Gamma (\kappa,\theta)$ with $\kappa \leq 1/2$, the density is given by 
 \[
	 \rho_0 (t) = \frac{1}{\Gamma(k)\theta ^{k}}t ^{k - 1} e ^{-t /\theta} \mathbf{1} _{t > 0}
 \]
 with $\Gamma(k) = \int_{0}^{\infty} s ^{\kappa - 1}e ^{-s} \diff s$. In particular, we have
 \[
	 \overline{F}(\Delta T _{k}) = 1 - \frac{\gamma(\kappa, \Delta T_k / \theta)}{\Gamma(k)}, \quad \gamma(\kappa,t) := \int_{0}^{t} s ^{\kappa - 1}e ^{-s} \diff t
 \]
 As a first step, we implement a naive solver based on Monte-Carlo sampling of \eqref{eq:representation} and use it on the toy problem presented in Section 5 of \cite{labordere2019branching}.
 We consider the case of $\mu = 0, \sigma = d ^{-1/2} I_d$ and $f(t,x,y,z) = k(t,x) + cy(b \cdot z)$ where
 \begin{align*}
	 b &= \frac{1}{d}(1 + \frac{1}{d}, 1 + \frac{2}{d}, \cdots, 2) \\
	 k(t,x) &= \cos (x_1 + \cdots + x_d) \left( \alpha + \frac{\sigma ^{2}}{2} + c \sin (x_1 + \cdots + x_d) \frac{3d + 1}{2d} e ^{\alpha(T - t)} \right) e ^{\alpha(T - t)}
 \end{align*}
 with terminal condition $g(x) = \cos (x_1 + \cdots + x_d)$. Then the explicit solution of \eqref{eq:pde} is given by
 \[
 u(t,x) = \cos (x_1 + \cdots + x_d) e ^{\alpha(T - t)}
 \]
 Note that in this case we have $L = \{(0,0), (1,1)\}$.

\bibliographystyle{plain}
\bibliography{references}
\end{document}
